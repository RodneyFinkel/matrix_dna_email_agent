{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b401f357",
   "metadata": {},
   "source": [
    "Fine tuning sentence-transformers/all-MiniLM-L12-v2 with  enron email dataset and LoRa with peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e442ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets torch peft pandas scikit-learn safetensors sentence-transformers\n",
    "%pip install \"numpy<2.0.0\"\n",
    "%pip install kaggle kagglehub\n",
    "%pip install seaborn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4e5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, Features, Value, ClassLabel\n",
    "import safetensors # Required for use_safetensors=True and safe_serialization=True\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import kagglehub\n",
    "\n",
    "# Force CPU\n",
    "torch.cuda.is_available = lambda: False\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ab6dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/wcukierski/enron-email-dataset?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 358M/358M [03:59<00:00, 1.56MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/rodneyfinkel/.cache/kagglehub/datasets/wcukierski/enron-email-dataset/versions/2\n"
     ]
    }
   ],
   "source": [
    "# download dataset, kaggle will download to cache \n",
    "path = kagglehub.dataset_download(\"wcukierski/enron-email-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4905e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied emails.csv to current working directory.\n",
      "Original cached dataset removed.\n"
     ]
    }
   ],
   "source": [
    "# move kaggle dataset to current working directory\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    src = os.path.join(path, filename)\n",
    "    dst = os.path.join(\".\", filename)\n",
    "    if os.path.isfile(src):\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"copied {filename} to current working directory.\")\n",
    "        # Remove the orginal cached dataset\n",
    "        cache_path = os.path.expanduser(\"~/.cache/kagglehub/datasets/wcukierski/enron-email-dataset\")\n",
    "        shutil.rmtree(cache_path)\n",
    "        print(\"Original cached dataset removed.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8201e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset\n",
    "df = pd.read_csv('emails.csv')\n",
    "df = df.sample(n=20000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "518b8bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>427616</th>\n",
       "      <td>shackleton-s/sent/1912.</td>\n",
       "      <td>Message-ID: &lt;21013688.1075844564560.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108773</th>\n",
       "      <td>farmer-d/logistics/1066.</td>\n",
       "      <td>Message-ID: &lt;22688499.1075854130303.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355471</th>\n",
       "      <td>parks-j/deleted_items/202.</td>\n",
       "      <td>Message-ID: &lt;27817771.1075841359502.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457837</th>\n",
       "      <td>stokley-c/chris_stokley/iso/client_rep/41.</td>\n",
       "      <td>Message-ID: &lt;10695160.1075858510449.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124910</th>\n",
       "      <td>germany-c/all_documents/1174.</td>\n",
       "      <td>Message-ID: &lt;27819143.1075853689038.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file  \\\n",
       "427616                     shackleton-s/sent/1912.   \n",
       "108773                    farmer-d/logistics/1066.   \n",
       "355471                  parks-j/deleted_items/202.   \n",
       "457837  stokley-c/chris_stokley/iso/client_rep/41.   \n",
       "124910               germany-c/all_documents/1174.   \n",
       "\n",
       "                                                  message  \n",
       "427616  Message-ID: <21013688.1075844564560.JavaMail.e...  \n",
       "108773  Message-ID: <22688499.1075854130303.JavaMail.e...  \n",
       "355471  Message-ID: <27817771.1075841359502.JavaMail.e...  \n",
       "457837  Message-ID: <10695160.1075858510449.JavaMail.e...  \n",
       "124910  Message-ID: <27819143.1075853689038.JavaMail.e...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31445ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean emails and extract subject and delete dates\n",
    "def clean_email(text):\n",
    "    # extract subject if available\n",
    "    subject_match = re.search(r'Subject: (.*?)\\n', text, re.IGNORECASE)\n",
    "    subject = subject_match.group(1) if subject_match else ''\n",
    "    # Remove headers, signatures and dates\n",
    "    text = re.sub(r'From:.*\\n|To:.*\\n|Subject:.*\\n|Message-ID:.*\\n|Date:.*\\n', '', text)\n",
    "    text = re.sub(r'-{2,}.*?-{2,}', '', text)\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    return text.strip(), subject\n",
    "\n",
    "df['message'], df['subject'] = zip(*df['message'].apply(clean_email))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c056d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>427616</th>\n",
       "      <td>shackleton-s/sent/1912.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>Re: Credit Derivatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108773</th>\n",
       "      <td>farmer-d/logistics/1066.</td>\n",
       "      <td>Cc: daren.farmer@enron.com\\nMime-Version: 1.0\\...</td>\n",
       "      <td>Meter #1591 Lamay Gaslift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355471</th>\n",
       "      <td>parks-j/deleted_items/202.</td>\n",
       "      <td>wollam.erik@enron.com, corrier.brad@enron.com\\...</td>\n",
       "      <td>Re: man night again?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457837</th>\n",
       "      <td>stokley-c/chris_stokley/iso/client_rep/41.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>Enron 480, 1480 charges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124910</th>\n",
       "      <td>germany-c/all_documents/1174.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>Transport Deal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file  \\\n",
       "427616                     shackleton-s/sent/1912.   \n",
       "108773                    farmer-d/logistics/1066.   \n",
       "355471                  parks-j/deleted_items/202.   \n",
       "457837  stokley-c/chris_stokley/iso/client_rep/41.   \n",
       "124910               germany-c/all_documents/1174.   \n",
       "\n",
       "                                                  message  \\\n",
       "427616  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "108773  Cc: daren.farmer@enron.com\\nMime-Version: 1.0\\...   \n",
       "355471  wollam.erik@enron.com, corrier.brad@enron.com\\...   \n",
       "457837  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "124910  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "\n",
       "                          subject  \n",
       "427616     Re: Credit Derivatives  \n",
       "108773  Meter #1591 Lamay Gaslift  \n",
       "355471       Re: man night again?  \n",
       "457837    Enron 480, 1480 charges  \n",
       "124910             Transport Deal  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1a5fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing spam emails from 6549 to 1000\n"
     ]
    }
   ],
   "source": [
    "# Heuristic spam labeling\n",
    "def is_potential_spam(text, subject):\n",
    "    spam_keywords = ['offer', 'free', 'win', 'click here', 'urgent', 'limited time', 'act now']\n",
    "    return any(keyword in text.lower() or keyword in subject.lower() for keyword in spam_keywords)\n",
    "\n",
    "df['label'] = df.apply(lambda x: is_potential_spam(x['message'], x['subject']), axis=1).astype(int)\n",
    "\n",
    "spam = df[df['label'] == 1]\n",
    "non_spam = df[df['label'] == 0]\n",
    "target_spam_count = int(len(df) * 0.05)\n",
    "\n",
    "if len(spam) > target_spam_count:\n",
    "    print(f\"Reducing spam emails from {len(spam)} to {target_spam_count}\")\n",
    "    spam = resample(spam, n_samples=target_spam_count, random_state=42)\n",
    "else:\n",
    "    non_spam = resample(non_spam, n_samples=int(target_spam_count * 3), random_state=42)\n",
    "\n",
    "df = pd.concat([spam, non_spam]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df.drop(columns=['label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74fa2583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14451, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sanders-r/all_documents/3872.</td>\n",
       "      <td>Cc: linda.guinn@enron.com, showard@milbank.com...</td>\n",
       "      <td>Re: NSM Docs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>haedicke-m/notes_inbox/359.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>Fw: Your Committee Minutes on Equity &amp; Energy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scott-s/gir/164.</td>\n",
       "      <td>johnj@bcjlaw.com, sscott3@enron.com, ghinners@...</td>\n",
       "      <td>Fwd: Exhibit 111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>derrick-j/inbox/200.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>Hello from Up State New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kean-s/sent_items/315.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>RE: all employee meeting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file  \\\n",
       "0  sanders-r/all_documents/3872.   \n",
       "1    haedicke-m/notes_inbox/359.   \n",
       "2               scott-s/gir/164.   \n",
       "3           derrick-j/inbox/200.   \n",
       "4         kean-s/sent_items/315.   \n",
       "\n",
       "                                             message  \\\n",
       "0  Cc: linda.guinn@enron.com, showard@milbank.com...   \n",
       "1  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "2  johnj@bcjlaw.com, sscott3@enron.com, ghinners@...   \n",
       "3  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "4  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "\n",
       "                                             subject  \n",
       "0                                      Re: NSM Docs.  \n",
       "1  Fw: Your Committee Minutes on Equity & Energy ...  \n",
       "2                                   Fwd: Exhibit 111  \n",
       "3                       Hello from Up State New York  \n",
       "4                           RE: all employee meeting  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec613611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic category labeling\n",
    "def assign_category(text, subject, file):\n",
    "    text = text.lower()\n",
    "    subject = subject.lower()\n",
    "    file = file.lower() \n",
    "    if any(kw in text or kw in subject for kw in ['budget', 'invoice', 'payment', 'financial']) or 'finance' in file or 'accounting' in file:\n",
    "        return 0  # Finance\n",
    "    elif any(kw in text or kw in subject for kw in ['hiring', 'employee', 'benefits', 'payroll']) or 'hr' in file or 'personnel' in file:\n",
    "        return 1  # HR\n",
    "    elif any(kw in text or kw in subject for kw in ['contract', 'legal', 'compliance', 'attorney']) or 'legal' in file:\n",
    "        return 2  # Legal\n",
    "    elif any(kw in text or kw in subject for kw in ['meeting', 'schedule', 'calendar', 'agenda']) or 'meetings' in file or 'admin' in file:\n",
    "        return 3  # Admin\n",
    "    return 0  # Default to finance (most common in Enron)\n",
    "\n",
    "df['category'] = df.apply(lambda x: assign_category(x['message'], x['subject'], x['file']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0126e1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "      <th>subject</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sanders-r/all_documents/3872.</td>\n",
       "      <td>Cc: linda.guinn@enron.com, showard@milbank.com...</td>\n",
       "      <td>Re: NSM Docs.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>haedicke-m/notes_inbox/359.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>Fw: Your Committee Minutes on Equity &amp; Energy ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scott-s/gir/164.</td>\n",
       "      <td>johnj@bcjlaw.com, sscott3@enron.com, ghinners@...</td>\n",
       "      <td>Fwd: Exhibit 111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>derrick-j/inbox/200.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>Hello from Up State New York</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kean-s/sent_items/315.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>RE: all employee meeting</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file  \\\n",
       "0  sanders-r/all_documents/3872.   \n",
       "1    haedicke-m/notes_inbox/359.   \n",
       "2               scott-s/gir/164.   \n",
       "3           derrick-j/inbox/200.   \n",
       "4         kean-s/sent_items/315.   \n",
       "\n",
       "                                             message  \\\n",
       "0  Cc: linda.guinn@enron.com, showard@milbank.com...   \n",
       "1  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "2  johnj@bcjlaw.com, sscott3@enron.com, ghinners@...   \n",
       "3  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "4  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "\n",
       "                                             subject  category  \n",
       "0                                      Re: NSM Docs.         1  \n",
       "1  Fw: Your Committee Minutes on Equity & Energy ...         3  \n",
       "2                                   Fwd: Exhibit 111         0  \n",
       "3                       Hello from Up State New York         0  \n",
       "4                           RE: all employee meeting         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a45496fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic priority labeling\n",
    "def assign_priority(text, subject):\n",
    "    text = text.lower()\n",
    "    subject = subject.lower()\n",
    "    if any(kw in text or kw in subject for kw in ['urgent', 'asap', 'deadline', 'immediate', 'action required']) or '!' in text:\n",
    "        return 0  # High\n",
    "    elif any(kw in text or kw in subject for kw in ['fyi', 'thanks', 'no rush']):\n",
    "        return 2  # Low\n",
    "    return 1  # Medium\n",
    "\n",
    "df['priority'] = df.apply(lambda x: assign_priority(x['message'], x['subject']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45f96e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "      <th>subject</th>\n",
       "      <th>category</th>\n",
       "      <th>priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sanders-r/all_documents/3872.</td>\n",
       "      <td>Cc: linda.guinn@enron.com, showard@milbank.com...</td>\n",
       "      <td>Re: NSM Docs.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>haedicke-m/notes_inbox/359.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>Fw: Your Committee Minutes on Equity &amp; Energy ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scott-s/gir/164.</td>\n",
       "      <td>johnj@bcjlaw.com, sscott3@enron.com, ghinners@...</td>\n",
       "      <td>Fwd: Exhibit 111</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>derrick-j/inbox/200.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>Hello from Up State New York</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kean-s/sent_items/315.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>RE: all employee meeting</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file  \\\n",
       "0  sanders-r/all_documents/3872.   \n",
       "1    haedicke-m/notes_inbox/359.   \n",
       "2               scott-s/gir/164.   \n",
       "3           derrick-j/inbox/200.   \n",
       "4         kean-s/sent_items/315.   \n",
       "\n",
       "                                             message  \\\n",
       "0  Cc: linda.guinn@enron.com, showard@milbank.com...   \n",
       "1  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "2  johnj@bcjlaw.com, sscott3@enron.com, ghinners@...   \n",
       "3  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "4  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "\n",
       "                                             subject  category  priority  \n",
       "0                                      Re: NSM Docs.         1         0  \n",
       "1  Fw: Your Committee Minutes on Equity & Energy ...         3         2  \n",
       "2                                   Fwd: Exhibit 111         0         2  \n",
       "3                       Hello from Up State New York         0         1  \n",
       "4                           RE: all employee meeting         0         1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4fd4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance categories (~25% each)\n",
    "category_counts = df['category'].value_counts()\n",
    "target_count = int(len(df) * 0.25)\n",
    "df_cat_balanced = pd.DataFrame()\n",
    "for cat in range(4):  # 0=finance, 1=hr, 2=legal, 3=admin\n",
    "    cat_df = df[df['category'] == cat]\n",
    "    if len(cat_df) > target_count:\n",
    "        cat_df = resample(cat_df, n_samples=target_count, random_state=42)\n",
    "    df_cat_balanced = pd.concat([df_cat_balanced, cat_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66ce0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance priorities (~33% each)\n",
    "priority_counts = df['priority'].value_counts()\n",
    "target_count = int(len(df) * 0.33)\n",
    "df_prio_balanced = pd.DataFrame()\n",
    "for prio in range(3):  # 0=high, 1=medium, 2=low\n",
    "    prio_df = df[df['priority'] == prio]\n",
    "    if len(prio_df) > target_count:\n",
    "        prio_df = resample(prio_df, n_samples=target_count, random_state=42)\n",
    "    df_prio_balanced = pd.concat([df_prio_balanced, prio_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "826f1e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14451, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "      <th>subject</th>\n",
       "      <th>category</th>\n",
       "      <th>priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sanders-r/all_documents/3872.</td>\n",
       "      <td>Cc: linda.guinn@enron.com, showard@milbank.com...</td>\n",
       "      <td>Re: NSM Docs.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>haedicke-m/notes_inbox/359.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>Fw: Your Committee Minutes on Equity &amp; Energy ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scott-s/gir/164.</td>\n",
       "      <td>johnj@bcjlaw.com, sscott3@enron.com, ghinners@...</td>\n",
       "      <td>Fwd: Exhibit 111</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>derrick-j/inbox/200.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>Hello from Up State New York</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kean-s/sent_items/315.</td>\n",
       "      <td>Mime-Version: 1.0\\nContent-Type: text/plain; c...</td>\n",
       "      <td>RE: all employee meeting</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file  \\\n",
       "0  sanders-r/all_documents/3872.   \n",
       "1    haedicke-m/notes_inbox/359.   \n",
       "2               scott-s/gir/164.   \n",
       "3           derrick-j/inbox/200.   \n",
       "4         kean-s/sent_items/315.   \n",
       "\n",
       "                                             message  \\\n",
       "0  Cc: linda.guinn@enron.com, showard@milbank.com...   \n",
       "1  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "2  johnj@bcjlaw.com, sscott3@enron.com, ghinners@...   \n",
       "3  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "4  Mime-Version: 1.0\\nContent-Type: text/plain; c...   \n",
       "\n",
       "                                             subject  category  priority  \n",
       "0                                      Re: NSM Docs.         1         0  \n",
       "1  Fw: Your Committee Minutes on Equity & Energy ...         3         2  \n",
       "2                                   Fwd: Exhibit 111         0         2  \n",
       "3                       Hello from Up State New York         0         1  \n",
       "4                           RE: all employee meeting         0         1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "def train_model(dataset, model_name, num_labels, output_dir, label_column, label_names):\n",
    "    # Combine subject and message for training\n",
    "    dataset['text'] = dataset.apply(lambda x: f\"[SUBJECT] {x['subject']} [BODY] {x['message']}\", axis=1)\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        dataset['text'], dataset[label_column], test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "     # Verify labels are integers\n",
    "    train_labels = train_labels.astype(int)\n",
    "    val_labels = val_labels.astype(int)\n",
    "    \n",
    "    # Define dataset features with ClassLabel\n",
    "    features = Features({\n",
    "        'text': Value('string'),\n",
    "        'label': ClassLabel(num_classes=num_labels, names=label_names)\n",
    "    })\n",
    "    \n",
    "    train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels}, features=features)\n",
    "    val_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels}, features=features)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Define id2label and label2id\n",
    "    id2label = {i: label for i, label in enumerate(label_names)}\n",
    "    label2id = {label: i for i, label in enumerate(label_names)}    \n",
    "    \n",
    "    # changed to use_safetensors = True\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id, use_safetensors=True)\n",
    "    \n",
    "    # Ensure model is in training mode and freeze base model parameters\n",
    "    model.train()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False) # Freeze base model\n",
    "    lora_config = LoraConfig(r=8, lora_alpha=16, target_modules=[\n",
    "        'query', 'key', 'value',  # Attention layers\n",
    "        'dense',  # Feed-forward layers\n",
    "        'classifier'  # Sequence classification head\n",
    "        \n",
    "        ], \n",
    "        lora_dropout=0.1, \n",
    "        bias=\"none\", \n",
    "        task_type=\"SEQ_CLS\",\n",
    "        modules_to_save=['classifier']  # Ensure classifier is saved\n",
    "        ) \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Enable gradient for both LoRA and classifier parameters\n",
    "    trainable_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(substr in name for substr in [\"lora\", \"classifier\"]):\n",
    "            param.requires_grad = True\n",
    "            trainable_params.append(name)\n",
    "    \n",
    "    # Debug: Print trainable parameters to verify\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128) # May need to lower max_length to 64 becuase of memory\n",
    "    \n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])  \n",
    "    \n",
    "    # Evaluation metrics\n",
    "    def compute_metrics(p):\n",
    "        preds = p.predictions.argmax(axis=-1)\n",
    "        labels = p.label_ids\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "            \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "        }\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=50,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=10,\n",
    "        eval_strategy='steps',\n",
    "        save_strategy='steps',\n",
    "        save_steps=200,\n",
    "        gradient_checkpointing=False, # set to False since LoRA handles memory optimization\n",
    "        fp16=False,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "     # Debug: Print training dataset features\n",
    "    print(f\"Training dataset features: {train_dataset.features}\")\n",
    "    \n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"Evaluation metrics for {output_dir}: {metrics}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    preds = trainer.predict(val_dataset)\n",
    "    cm = confusion_matrix(preds.label_ids, preds.predictions.argmax(axis=1))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.title(f'Confusion Matrix - {output_dir}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    model.save_pretrained(output_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226a99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.output.dense.lora_B.default.weight', 'base_model.model.bert.pooler.dense.lora_A.default.weight', 'base_model.model.bert.pooler.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.classifier.modules_to_save.default.bias']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa10c212872d4b78821ad8edbe5c3cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7996ed7e284992b6e97b10f9f2440a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset features: {'text': Value('string'), 'label': ClassLabel(names=['Finance', 'HR', 'Legal', 'Admin']), 'input_ids': List(Value('int32')), 'token_type_ids': List(Value('int8')), 'attention_mask': List(Value('int8'))}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1021' max='5553' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1021/5553 16:41:20 < 74:13:28, 0.02 it/s, Epoch 0.55/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.390900</td>\n",
       "      <td>1.386376</td>\n",
       "      <td>0.244192</td>\n",
       "      <td>0.164412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.383500</td>\n",
       "      <td>1.385700</td>\n",
       "      <td>0.260940</td>\n",
       "      <td>0.187482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.387100</td>\n",
       "      <td>1.385150</td>\n",
       "      <td>0.273906</td>\n",
       "      <td>0.203970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.383100</td>\n",
       "      <td>1.383995</td>\n",
       "      <td>0.306861</td>\n",
       "      <td>0.239736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.381259</td>\n",
       "      <td>0.392761</td>\n",
       "      <td>0.265216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.383300</td>\n",
       "      <td>1.377555</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.373500</td>\n",
       "      <td>1.374453</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.371800</td>\n",
       "      <td>1.370963</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.380300</td>\n",
       "      <td>1.368014</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.364100</td>\n",
       "      <td>1.364220</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.350900</td>\n",
       "      <td>1.357416</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.337800</td>\n",
       "      <td>1.350596</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.363300</td>\n",
       "      <td>1.345151</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.269100</td>\n",
       "      <td>1.339117</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.312400</td>\n",
       "      <td>1.334413</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.296400</td>\n",
       "      <td>1.331494</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.423000</td>\n",
       "      <td>1.330019</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.278000</td>\n",
       "      <td>1.329161</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.321900</td>\n",
       "      <td>1.327542</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.310600</td>\n",
       "      <td>1.327833</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.361600</td>\n",
       "      <td>1.326258</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.319000</td>\n",
       "      <td>1.323226</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.326100</td>\n",
       "      <td>1.321989</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.291600</td>\n",
       "      <td>1.319044</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.309600</td>\n",
       "      <td>1.320348</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.340300</td>\n",
       "      <td>1.322962</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.226700</td>\n",
       "      <td>1.324779</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.332100</td>\n",
       "      <td>1.324555</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.342600</td>\n",
       "      <td>1.320787</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.352900</td>\n",
       "      <td>1.317348</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.282900</td>\n",
       "      <td>1.315570</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.277600</td>\n",
       "      <td>1.314819</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.270400</td>\n",
       "      <td>1.316406</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.309400</td>\n",
       "      <td>1.317619</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.389300</td>\n",
       "      <td>1.313117</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.233600</td>\n",
       "      <td>1.309840</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.357700</td>\n",
       "      <td>1.308923</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.366700</td>\n",
       "      <td>1.307922</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.316300</td>\n",
       "      <td>1.310536</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.316500</td>\n",
       "      <td>1.307168</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.440500</td>\n",
       "      <td>1.307419</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.337700</td>\n",
       "      <td>1.312014</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.281600</td>\n",
       "      <td>1.304293</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.363700</td>\n",
       "      <td>1.302538</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.327000</td>\n",
       "      <td>1.303168</td>\n",
       "      <td>0.391680</td>\n",
       "      <td>0.220472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.315800</td>\n",
       "      <td>1.304169</td>\n",
       "      <td>0.392220</td>\n",
       "      <td>0.221636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.369400</td>\n",
       "      <td>1.319985</td>\n",
       "      <td>0.418693</td>\n",
       "      <td>0.276125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.340800</td>\n",
       "      <td>1.325748</td>\n",
       "      <td>0.414911</td>\n",
       "      <td>0.267454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.351800</td>\n",
       "      <td>1.311562</td>\n",
       "      <td>0.407347</td>\n",
       "      <td>0.253940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.337000</td>\n",
       "      <td>1.297697</td>\n",
       "      <td>0.412750</td>\n",
       "      <td>0.263679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.357700</td>\n",
       "      <td>1.294391</td>\n",
       "      <td>0.432199</td>\n",
       "      <td>0.297999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.345800</td>\n",
       "      <td>1.290845</td>\n",
       "      <td>0.432199</td>\n",
       "      <td>0.303571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.262100</td>\n",
       "      <td>1.284359</td>\n",
       "      <td>0.432199</td>\n",
       "      <td>0.299111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.316900</td>\n",
       "      <td>1.280287</td>\n",
       "      <td>0.426256</td>\n",
       "      <td>0.288315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.347700</td>\n",
       "      <td>1.275741</td>\n",
       "      <td>0.420854</td>\n",
       "      <td>0.278075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.183700</td>\n",
       "      <td>1.269370</td>\n",
       "      <td>0.421934</td>\n",
       "      <td>0.281526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.289700</td>\n",
       "      <td>1.260863</td>\n",
       "      <td>0.438142</td>\n",
       "      <td>0.310732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.327800</td>\n",
       "      <td>1.254153</td>\n",
       "      <td>0.469476</td>\n",
       "      <td>0.379240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.299400</td>\n",
       "      <td>1.252391</td>\n",
       "      <td>0.549973</td>\n",
       "      <td>0.485019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.290300</td>\n",
       "      <td>1.244916</td>\n",
       "      <td>0.561318</td>\n",
       "      <td>0.496437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.238500</td>\n",
       "      <td>1.232150</td>\n",
       "      <td>0.564560</td>\n",
       "      <td>0.499323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.288000</td>\n",
       "      <td>1.222982</td>\n",
       "      <td>0.565100</td>\n",
       "      <td>0.499031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.127300</td>\n",
       "      <td>1.214275</td>\n",
       "      <td>0.561858</td>\n",
       "      <td>0.490597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.249300</td>\n",
       "      <td>1.210695</td>\n",
       "      <td>0.560778</td>\n",
       "      <td>0.488152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.401200</td>\n",
       "      <td>1.203581</td>\n",
       "      <td>0.579687</td>\n",
       "      <td>0.513021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.299100</td>\n",
       "      <td>1.221710</td>\n",
       "      <td>0.568341</td>\n",
       "      <td>0.500289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.146600</td>\n",
       "      <td>1.191834</td>\n",
       "      <td>0.568882</td>\n",
       "      <td>0.496624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.315200</td>\n",
       "      <td>1.177574</td>\n",
       "      <td>0.565640</td>\n",
       "      <td>0.490111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.356100</td>\n",
       "      <td>1.185749</td>\n",
       "      <td>0.554295</td>\n",
       "      <td>0.478903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.245400</td>\n",
       "      <td>1.191206</td>\n",
       "      <td>0.537547</td>\n",
       "      <td>0.457567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.098900</td>\n",
       "      <td>1.161514</td>\n",
       "      <td>0.543490</td>\n",
       "      <td>0.458493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.171300</td>\n",
       "      <td>1.148921</td>\n",
       "      <td>0.544571</td>\n",
       "      <td>0.455668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.145700</td>\n",
       "      <td>1.139332</td>\n",
       "      <td>0.531605</td>\n",
       "      <td>0.437161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.203400</td>\n",
       "      <td>1.139179</td>\n",
       "      <td>0.525662</td>\n",
       "      <td>0.435885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.241600</td>\n",
       "      <td>1.137488</td>\n",
       "      <td>0.523501</td>\n",
       "      <td>0.431980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.166800</td>\n",
       "      <td>1.117417</td>\n",
       "      <td>0.549433</td>\n",
       "      <td>0.463106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.111000</td>\n",
       "      <td>1.111221</td>\n",
       "      <td>0.551053</td>\n",
       "      <td>0.468481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.994500</td>\n",
       "      <td>1.104844</td>\n",
       "      <td>0.546731</td>\n",
       "      <td>0.459441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.150700</td>\n",
       "      <td>1.100610</td>\n",
       "      <td>0.545111</td>\n",
       "      <td>0.455140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.074600</td>\n",
       "      <td>1.096704</td>\n",
       "      <td>0.539168</td>\n",
       "      <td>0.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.158600</td>\n",
       "      <td>1.104376</td>\n",
       "      <td>0.543490</td>\n",
       "      <td>0.445422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.081800</td>\n",
       "      <td>1.097644</td>\n",
       "      <td>0.540249</td>\n",
       "      <td>0.439711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.042200</td>\n",
       "      <td>1.089475</td>\n",
       "      <td>0.534306</td>\n",
       "      <td>0.431050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.198300</td>\n",
       "      <td>1.086215</td>\n",
       "      <td>0.531605</td>\n",
       "      <td>0.424816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.080478</td>\n",
       "      <td>0.526742</td>\n",
       "      <td>0.420203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.046900</td>\n",
       "      <td>1.081507</td>\n",
       "      <td>0.521340</td>\n",
       "      <td>0.421170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.990900</td>\n",
       "      <td>1.077899</td>\n",
       "      <td>0.525662</td>\n",
       "      <td>0.424105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.052300</td>\n",
       "      <td>1.075966</td>\n",
       "      <td>0.523501</td>\n",
       "      <td>0.419628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.118700</td>\n",
       "      <td>1.075721</td>\n",
       "      <td>0.518639</td>\n",
       "      <td>0.416651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.084700</td>\n",
       "      <td>1.074458</td>\n",
       "      <td>0.527823</td>\n",
       "      <td>0.429725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.954500</td>\n",
       "      <td>1.075140</td>\n",
       "      <td>0.534846</td>\n",
       "      <td>0.438260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.162200</td>\n",
       "      <td>1.075139</td>\n",
       "      <td>0.531605</td>\n",
       "      <td>0.436238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.222500</td>\n",
       "      <td>1.075525</td>\n",
       "      <td>0.539168</td>\n",
       "      <td>0.448371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.101700</td>\n",
       "      <td>1.067348</td>\n",
       "      <td>0.539708</td>\n",
       "      <td>0.448584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.903400</td>\n",
       "      <td>1.065565</td>\n",
       "      <td>0.533766</td>\n",
       "      <td>0.444152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.087500</td>\n",
       "      <td>1.062144</td>\n",
       "      <td>0.525122</td>\n",
       "      <td>0.431989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.114000</td>\n",
       "      <td>1.062325</td>\n",
       "      <td>0.530524</td>\n",
       "      <td>0.438454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.944200</td>\n",
       "      <td>1.063195</td>\n",
       "      <td>0.527283</td>\n",
       "      <td>0.435587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.159200</td>\n",
       "      <td>1.061291</td>\n",
       "      <td>0.531064</td>\n",
       "      <td>0.437896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.853700</td>\n",
       "      <td>1.065781</td>\n",
       "      <td>0.546731</td>\n",
       "      <td>0.457645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.107600</td>\n",
       "      <td>1.073064</td>\n",
       "      <td>0.561858</td>\n",
       "      <td>0.479774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='179' max='463' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [179/463 02:01 < 03:13, 1.47 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodneyfinkel/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/peft/utils/other.py:1221: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 865b3e15-424d-448e-a295-e555d7605376)') - silently ignoring the lookup for the file config.json in sentence-transformers/all-MiniLM-L12-v2.\n",
      "  warnings.warn(\n",
      "/Users/rodneyfinkel/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/peft/utils/save_and_load.py:238: UserWarning: Could not find a config file in sentence-transformers/all-MiniLM-L12-v2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train category classifier\n",
    "# changed model to sentence-transformers/all-MiniLM-L12-v2 from microsoft/minilm-l12-h384-uncased\n",
    "category_label_names = [\"Finance\", \"HR\", \"Legal\", \"Admin\"]\n",
    "model_cat, tokenizer_cat = train_model(\n",
    "    df_cat_balanced, 'sentence-transformers/all-MiniLM-L12-v2', 4, './fine_tuned_minilm_category', 'category', category_label_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94cbb46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.attention.output.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.intermediate.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.intermediate.dense.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.output.dense.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.output.dense.lora_B.default.weight', 'base_model.model.bert.pooler.dense.lora_A.default.weight', 'base_model.model.bert.pooler.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.classifier.modules_to_save.default.bias']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1822aa48c3194682be9b357a3ce29312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d17545d5a5470bab860bfb78c5f0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset features: {'text': Value('string'), 'label': ClassLabel(names=['High', 'Medium', 'Low']), 'input_ids': List(Value('int32')), 'token_type_ids': List(Value('int8')), 'attention_mask': List(Value('int8'))}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='811' max='6708' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 811/6708 9:01:56 < 65:50:23, 0.02 it/s, Epoch 0.36/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.102800</td>\n",
       "      <td>1.100014</td>\n",
       "      <td>0.273703</td>\n",
       "      <td>0.196321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.099300</td>\n",
       "      <td>1.099821</td>\n",
       "      <td>0.274150</td>\n",
       "      <td>0.210412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.098800</td>\n",
       "      <td>1.099716</td>\n",
       "      <td>0.275045</td>\n",
       "      <td>0.194063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.097700</td>\n",
       "      <td>1.099623</td>\n",
       "      <td>0.276386</td>\n",
       "      <td>0.190018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.098868</td>\n",
       "      <td>0.309034</td>\n",
       "      <td>0.259440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.099700</td>\n",
       "      <td>1.097073</td>\n",
       "      <td>0.412343</td>\n",
       "      <td>0.284342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.095500</td>\n",
       "      <td>1.095820</td>\n",
       "      <td>0.418157</td>\n",
       "      <td>0.253939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.100100</td>\n",
       "      <td>1.095433</td>\n",
       "      <td>0.419946</td>\n",
       "      <td>0.253808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.091300</td>\n",
       "      <td>1.094063</td>\n",
       "      <td>0.422182</td>\n",
       "      <td>0.251264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.095300</td>\n",
       "      <td>1.092687</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.093400</td>\n",
       "      <td>1.091768</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.103300</td>\n",
       "      <td>1.091748</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.096300</td>\n",
       "      <td>1.091701</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.084900</td>\n",
       "      <td>1.090815</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.083300</td>\n",
       "      <td>1.088360</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.099100</td>\n",
       "      <td>1.087020</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.086800</td>\n",
       "      <td>1.086990</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.096700</td>\n",
       "      <td>1.086891</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.084800</td>\n",
       "      <td>1.085518</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.091100</td>\n",
       "      <td>1.084357</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.068600</td>\n",
       "      <td>1.082551</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.062100</td>\n",
       "      <td>1.080128</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.091300</td>\n",
       "      <td>1.078827</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.089800</td>\n",
       "      <td>1.078295</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.042600</td>\n",
       "      <td>1.077277</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.091200</td>\n",
       "      <td>1.076934</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.081100</td>\n",
       "      <td>1.076487</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.047100</td>\n",
       "      <td>1.076260</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.001800</td>\n",
       "      <td>1.078822</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.110700</td>\n",
       "      <td>1.079114</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.080300</td>\n",
       "      <td>1.078945</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.006100</td>\n",
       "      <td>1.081257</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.067900</td>\n",
       "      <td>1.082335</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.051700</td>\n",
       "      <td>1.084145</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.024300</td>\n",
       "      <td>1.085010</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.057700</td>\n",
       "      <td>1.084311</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.193700</td>\n",
       "      <td>1.079116</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.034300</td>\n",
       "      <td>1.077674</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.104000</td>\n",
       "      <td>1.076551</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.077400</td>\n",
       "      <td>1.075841</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.122100</td>\n",
       "      <td>1.075484</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.063300</td>\n",
       "      <td>1.075662</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.056000</td>\n",
       "      <td>1.075504</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.086100</td>\n",
       "      <td>1.075649</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.010600</td>\n",
       "      <td>1.076231</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.057200</td>\n",
       "      <td>1.078878</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.071700</td>\n",
       "      <td>1.080747</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.172900</td>\n",
       "      <td>1.077575</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.086200</td>\n",
       "      <td>1.076446</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.015600</td>\n",
       "      <td>1.076694</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.041600</td>\n",
       "      <td>1.077971</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.123300</td>\n",
       "      <td>1.077926</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.029600</td>\n",
       "      <td>1.077742</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.106700</td>\n",
       "      <td>1.076988</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.083100</td>\n",
       "      <td>1.075443</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.110800</td>\n",
       "      <td>1.074682</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.111900</td>\n",
       "      <td>1.074478</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.095100</td>\n",
       "      <td>1.074361</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.059400</td>\n",
       "      <td>1.073913</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.049100</td>\n",
       "      <td>1.073562</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>1.073233</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.061400</td>\n",
       "      <td>1.073095</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.085400</td>\n",
       "      <td>1.072984</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.133600</td>\n",
       "      <td>1.073061</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.117500</td>\n",
       "      <td>1.074390</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.093600</td>\n",
       "      <td>1.074940</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.049100</td>\n",
       "      <td>1.074803</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.070500</td>\n",
       "      <td>1.074240</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.062100</td>\n",
       "      <td>1.072908</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.082900</td>\n",
       "      <td>1.072477</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.036600</td>\n",
       "      <td>1.072015</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.052100</td>\n",
       "      <td>1.072318</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.097500</td>\n",
       "      <td>1.072937</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.982400</td>\n",
       "      <td>1.074626</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.107100</td>\n",
       "      <td>1.076584</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.989500</td>\n",
       "      <td>1.075223</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.936300</td>\n",
       "      <td>1.082998</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.119200</td>\n",
       "      <td>1.081598</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.122700</td>\n",
       "      <td>1.074212</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.133300</td>\n",
       "      <td>1.071176</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.251559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43' max='559' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 43/559 00:27 < 05:41, 1.51 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train priority classifier\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# changed model to sentence-transformers/all-MiniLM-L12-v2 from microsoft/minilm-l12-h384-uncased\u001b[39;00m\n\u001b[32m      3\u001b[39m priority_label_names = [\u001b[33m\"\u001b[39m\u001b[33mHigh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMedium\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLow\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model_prio, tokenizer_prio = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_prio_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentence-transformers/all-MiniLM-L12-v2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./fine_tuned_minilm_priority\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpriority\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriority_label_names\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(dataset, model_name, num_labels, output_dir, label_column, label_names)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining dataset features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dataset.features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m metrics = trainer.evaluate()\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluation metrics for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/trainer.py:2623\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2621\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2622\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2623\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2634\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/trainer.py:3096\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3094\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3096\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3097\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3099\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/trainer.py:3045\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3044\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3048\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/trainer.py:4199\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4196\u001b[39m start_time = time.time()\n\u001b[32m   4198\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4199\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4200\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4202\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4207\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4209\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/trainer.py:4394\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4391\u001b[39m         batch_size = observed_batch_size\n\u001b[32m   4393\u001b[39m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4394\u001b[39m losses, logits, labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4395\u001b[39m main_input_name = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmain_input_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4396\u001b[39m inputs_decode = (\n\u001b[32m   4397\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4398\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/trainer.py:4610\u001b[39m, in \u001b[36mTrainer.prediction_step\u001b[39m\u001b[34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[39m\n\u001b[32m   4608\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[32m   4609\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4610\u001b[39m         loss, outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   4611\u001b[39m     loss = loss.detach().mean()\n\u001b[32m   4613\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/trainer.py:3836\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3834\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3835\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3836\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3837\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3838\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3839\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/peft/peft_model.py:1647\u001b[39m, in \u001b[36mPeftModelForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1645\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m peft_config.peft_type == PeftType.POLY:\n\u001b[32m   1646\u001b[39m             kwargs[\u001b[33m\"\u001b[39m\u001b[33mtask_ids\u001b[39m\u001b[33m\"\u001b[39m] = task_ids\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1656\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1658\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:216\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1483\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1475\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1476\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1477\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1478\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1479\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1480\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1481\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1483\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1484\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1495\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1497\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    990\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    991\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    994\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1008\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1009\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:651\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    648\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    649\u001b[39m past_key_value = past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         logger.warning(message)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:553\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    542\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    543\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    550\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    552\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    562\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:492\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    474\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    475\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    482\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m    483\u001b[39m     self_outputs = \u001b[38;5;28mself\u001b[39m.self(\n\u001b[32m    484\u001b[39m         hidden_states,\n\u001b[32m    485\u001b[39m         attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m         output_attentions,\n\u001b[32m    491\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:434\u001b[39m, in \u001b[36mBertSelfOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    436\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/peft/tuners/lora/layer.py:755\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    753\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.base_layer(x, *args, **kwargs)\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m755\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m     torch_result_dtype = result.dtype\n\u001b[32m    758\u001b[39m     lora_A_keys = \u001b[38;5;28mself\u001b[39m.lora_A.keys()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/matrix_dna_email_agent/env/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train priority classifier\n",
    "# changed model to sentence-transformers/all-MiniLM-L12-v2 from microsoft/minilm-l12-h384-uncased\n",
    "priority_label_names = [\"High\", \"Medium\", \"Low\"]\n",
    "model_prio, tokenizer_prio = train_model(\n",
    "    df_prio_balanced, 'sentence-transformers/all-MiniLM-L12-v2', 3, './fine_tuned_minilm_priority', 'priority', priority_label_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b72e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference functions\n",
    "def predict_category(email_text, model=model_cat, tokenizer=tokenizer_cat):\n",
    "    text, subject = clean_email(email_text)\n",
    "    input_text = f\"[SUBJECT] {subject} [BODY] {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return category_label_names[predicted_class]\n",
    "\n",
    "def predict_priority(email_text, model=model_prio, tokenizer=tokenizer_prio):\n",
    "    text, subject = clean_email(email_text)\n",
    "    input_text = f\"[SUBJECT] {subject} [BODY] {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return priority_label_names[predicted_class]\n",
    "\n",
    "# Test inference\n",
    "sample_email = \"Subject: Budget Review\\nDate: Wed, 29 Nov 2000 05:40:00 -0800\\nPlease review the attached budget for Q3.\"\n",
    "print(f\"Category: {predict_category(sample_email)}\")\n",
    "print(f\"Priority: {predict_priority(sample_email)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
